{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import re\n",
                "import random\n",
                "import time\n",
                "from collections import Counter\n",
                "\n",
                "urls = [\n",
                "    \"https://ka.wikipedia.org/wiki/საქართველო\",\n",
                "    \"https://ka.wikipedia.org/wiki/თბილისი\",\n",
                "    \"https://ka.wikipedia.org/wiki/ვეფხისტყაოსანი\",\n",
                "    \"https://ka.wikipedia.org/wiki/ილია_ჭავჭავაძე\",\n",
                "    \"https://ka.wikipedia.org/wiki/ქართული_ენა\",\n",
                "]\n",
                "\n",
                "def fetch_text(url):\n",
                "    try:\n",
                "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
                "        r = requests.get(url, headers=headers)\n",
                "        if r.status_code == 200:\n",
                "            return r.text\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching {url}: {e}\")\n",
                "    return \"\"\n",
                "\n",
                "raw_text = \"\"\n",
                "for url in urls:\n",
                "    raw_text += fetch_text(url) + \" \"\n",
                "    time.sleep(1)\n",
                "\n",
                "def clean_and_tokenize(html_text):\n",
                "    text = re.sub(r'<[^>]+>', ' ', html_text)\n",
                "    words = re.findall(r'[ა-ჰ]+', text)\n",
                "    return words\n",
                "\n",
                "all_words = clean_and_tokenize(raw_text)\n",
                "unique_words = list(set(word for word in all_words if len(word) > 2))\n",
                "\n",
                "# Fallback if scraping fails\n",
                "if len(unique_words) < 10:\n",
                "    print(\"Scraping failed or returned few words. Using fallback list.\")\n",
                "    unique_words = [\n",
                "        \"გამარჯობა\", \"საქართველო\", \"თბილისი\", \"ქუთაისი\", \"ბათუმი\", \n",
                "        \"რუსთავი\", \"გორი\", \"ზუგდიდი\", \"ფოთი\", \"ხაშური\", \n",
                "        \"სამტრედია\", \"სენაკი\", \"თელავი\", \"ახალციხე\", \"ქობულეთი\",\n",
                "        \"ოზურგეთი\", \"კასპი\", \"ჭიათურა\", \"წყალტუბო\", \"საგარეჯო\",\n",
                "        \"გარდაბანი\", \"ბორჯომი\", \"ტყიბული\", \"ხონი\", \"ბოლნისი\",\n",
                "        \"ახალქალაქი\", \"გურჯაანი\", \"მცხეთა\", \"ყვარელი\", \"ახმეტა\",\n",
                "        \"კარელი\", \"ლანჩხუთი\", \"დუშეთი\", \"საჩხერე\", \"დედოფლისწყარო\",\n",
                "        \"ლაგოდეხი\", \"ნინოწმინდა\", \"აბაშა\", \"წნორი\", \"ვალე\",\n",
                "        \"ჯვარი\", \"თეთრიწყარო\", \"სიღნაღი\", \"ბაღდათი\", \"ვანი\"\n",
                "    ]\n",
                "\n",
                "print(f\"Total words found: {len(all_words)}\")\n",
                "print(f\"Unique vocabulary size: {len(unique_words)}\")\n",
                "print(\"Examples:\", unique_words[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "georgian_chars = sorted(list(set(\" \".join(unique_words))))\n",
                "\n",
                "def inject_noise(word, error_rate=0.7):\n",
                "    if random.random() > error_rate:\n",
                "        return word\n",
                "    \n",
                "    chars = list(word)\n",
                "    if not chars: return word\n",
                "    \n",
                "    op = random.choice(['insert', 'delete', 'replace', 'swap'])\n",
                "    idx = random.randint(0, len(chars) - 1)\n",
                "    \n",
                "    if op == 'insert':\n",
                "        chars.insert(idx, random.choice(georgian_chars))\n",
                "    elif op == 'delete':\n",
                "        if len(chars) > 1:\n",
                "            chars.pop(idx)\n",
                "    elif op == 'replace':\n",
                "        chars[idx] = random.choice(georgian_chars)\n",
                "    elif op == 'swap':\n",
                "        if idx < len(chars) - 1:\n",
                "            chars[idx], chars[idx+1] = chars[idx+1], chars[idx]\n",
                "            \n",
                "    return \"\".join(chars)\n",
                "\n",
                "dataset_pairs = []\n",
                "\n",
                "for word in unique_words:\n",
                "    dataset_pairs.append((word, word))\n",
                "    for _ in range(3):\n",
                "        corrupted = inject_noise(word, error_rate=1.0)\n",
                "        dataset_pairs.append((corrupted, word))\n",
                "\n",
                "random.shuffle(dataset_pairs)\n",
                "print(f\"Total training pairs: {len(dataset_pairs)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np\n",
                "\n",
                "SOS_token = 0\n",
                "EOS_token = 1\n",
                "PAD_token = 2\n",
                "\n",
                "class Vocabulary:\n",
                "    def __init__(self):\n",
                "        self.char2index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2}\n",
                "        self.index2char = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<PAD>\"}\n",
                "        self.n_chars = 3\n",
                "\n",
                "    def add_word(self, word):\n",
                "        for char in word:\n",
                "            if char not in self.char2index:\n",
                "                self.char2index[char] = self.n_chars\n",
                "                self.index2char[self.n_chars] = char\n",
                "                self.n_chars += 1\n",
                "\n",
                "vocab = Vocabulary()\n",
                "for _, word in dataset_pairs:\n",
                "    vocab.add_word(word)\n",
                "for word, _ in dataset_pairs:\n",
                "    vocab.add_word(word)\n",
                "\n",
                "MAX_LENGTH = max(len(p[0]) for p in dataset_pairs) + 5\n",
                "\n",
                "class SpellDataset(Dataset):\n",
                "    def __init__(self, pairs, vocab):\n",
                "        self.pairs = pairs\n",
                "        self.vocab = vocab\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.pairs)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        input_word, target_word = self.pairs[idx]\n",
                "        return self.tensorFromWord(input_word), self.tensorFromWord(target_word)\n",
                "\n",
                "    def tensorFromWord(self, word):\n",
                "        indexes = [self.vocab.char2index[char] for char in word]\n",
                "        indexes.append(EOS_token)\n",
                "        if len(indexes) < MAX_LENGTH:\n",
                "            indexes += [PAD_token] * (MAX_LENGTH - len(indexes))\n",
                "        else:\n",
                "            indexes = indexes[:MAX_LENGTH-1] + [EOS_token]\n",
                "        return torch.tensor(indexes, dtype=torch.long)\n",
                "\n",
                "split_idx = int(0.9 * len(dataset_pairs))\n",
                "train_pairs = dataset_pairs[:split_idx]\n",
                "val_pairs = dataset_pairs[split_idx:]\n",
                "\n",
                "train_dataset = SpellDataset(train_pairs, vocab)\n",
                "val_dataset = SpellDataset(val_pairs, vocab)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderRNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
                "        super(EncoderRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
                "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, input):\n",
                "        embedded = self.dropout(self.embedding(input))\n",
                "        output, hidden = self.gru(embedded)\n",
                "        return output, hidden\n",
                "\n",
                "class DecoderRNN(nn.Module):\n",
                "    def __init__(self, hidden_size, output_size, dropout=0.1):\n",
                "        super(DecoderRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
                "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "        self.out = nn.Linear(hidden_size, output_size)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, input, hidden):\n",
                "        output = self.dropout(self.embedding(input))\n",
                "        output = torch.nn.functional.relu(output)\n",
                "        output, hidden = self.gru(output, hidden)\n",
                "        output = self.out(output)\n",
                "        return output, hidden\n",
                "\n",
                "class Seq2Seq(nn.Module):\n",
                "    def __init__(self, encoder, decoder, device):\n",
                "        super().__init__()\n",
                "        self.encoder = encoder\n",
                "        self.decoder = decoder\n",
                "        self.device = device\n",
                "\n",
                "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
                "        batch_size = src.shape[0]\n",
                "        trg_len = trg.shape[1]\n",
                "        trg_vocab_size = self.decoder.out.out_features\n",
                "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
                "        _, hidden = self.encoder(src)\n",
                "        input = trg[:, 0].unsqueeze(1)\n",
                "        for t in range(1, trg_len):\n",
                "            output, hidden = self.decoder.gru(self.decoder.dropout(self.decoder.embedding(input)), hidden)\n",
                "            prediction = self.decoder.out(output)\n",
                "\n",
                "            outputs[:, t, :] = prediction.squeeze(1)\n",
                "            teacher_force = random.random() < teacher_forcing_ratio\n",
                "            top1 = prediction.argmax(2)\n",
                "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
                "\n",
                "        return outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "HIDDEN_SIZE = 256\n",
                "enc = EncoderRNN(vocab.n_chars, HIDDEN_SIZE).to(device)\n",
                "dec = DecoderRNN(HIDDEN_SIZE, vocab.n_chars).to(device)\n",
                "model = Seq2Seq(enc, dec, device).to(device)\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
                "\n",
                "def train(model, iterator, optimizer, criterion, clip):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    for i, (src, trg) in enumerate(iterator):\n",
                "        src, trg = src.to(device), trg.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        output = model(src, trg)\n",
                "        output_dim = output.shape[-1]\n",
                "        \n",
                "        output = output[:, 1:].reshape(-1, output_dim)\n",
                "        trg = trg[:, 1:].reshape(-1)\n",
                "        \n",
                "        loss = criterion(output, trg)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    return epoch_loss / len(iterator)\n",
                "\n",
                "def evaluate(model, iterator, criterion):\n",
                "    model.eval()\n",
                "    epoch_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for i, (src, trg) in enumerate(iterator):\n",
                "            src, trg = src.to(device), trg.to(device)\n",
                "            output = model(src, trg, 0)\n",
                "            output_dim = output.shape[-1]\n",
                "            output = output[:, 1:].reshape(-1, output_dim)\n",
                "            trg = trg[:, 1:].reshape(-1)\n",
                "            loss = criterion(output, trg)\n",
                "            epoch_loss += loss.item()\n",
                "    return epoch_loss / len(iterator)\n",
                "\n",
                "N_EPOCHS = 15\n",
                "CLIP = 1\n",
                "best_valid_loss = float('inf')\n",
                "\n",
                "for epoch in range(N_EPOCHS):\n",
                "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
                "    valid_loss = evaluate(model, val_loader, criterion)\n",
                "    if valid_loss < best_valid_loss:\n",
                "        best_valid_loss = valid_loss\n",
                "        torch.save(model.state_dict(), 'models/best_model.pt')\n",
                "    \n",
                "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "save_data = {\n",
                "    'char2index': vocab.char2index,\n",
                "    'index2char': vocab.index2char,\n",
                "    'hidden_size': HIDDEN_SIZE,\n",
                "    'n_chars': vocab.n_chars,\n",
                "    'max_length': MAX_LENGTH\n",
                "}\n",
                "\n",
                "with open('models/model_config.pkl', 'wb') as f:\n",
                "    pickle.dump(save_data, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}